{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJdwKu1uq5FjUTkOWCaTMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tr524/AzureAI-Foundry/blob/main/Practice_activity_Auditing_ML_code_for_security_vulnerabilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this activity, you will be tasked with reviewing a block of intentionally flawed ML code. Your objective is to audit the code for potential security vulnerabilities that could pose risks to data integrity, confidentiality, and the overall security of the ML system. This activity will take you approximately 60 minutes to complete.\n",
        "\n",
        "By the end of this activity, you will be able to:\n",
        "\n",
        "Identify and mitigate security risks in AI/ML development.\n",
        "\n",
        "Explain the implications of security vulnerabilities in ML code.\n",
        "\n",
        "Propose solutions to mitigate the identified risks."
      ],
      "metadata": {
        "id": "gnFpu_u2S2dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-step guide\n",
        "Step 1: Review the provided ML code block\n",
        "You will be given a block of ML code that contains several intentional security flaws. Your first task is to carefully review the code and identify any potential vulnerabilities. These may include issues related to data handling, access controls, model deployment, and other security aspects.\n",
        "\n",
        "Code block example (with intentional flaws)"
      ],
      "metadata": {
        "id": "mWW_zHfIS-Ho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_cg7gpRS1lt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "# Load dataset (Flaw: No data validation or sanitization)\n",
        "data = pd.read_csv('user_data.csv')\n",
        "# Split the dataset into features and target (Flaw: No input validation)\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "# Split the data into training and testing sets (Flaw: Fixed random state)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Train a simple logistic regression model (Flaw: No model security checks)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Save the model to disk (Flaw: Unencrypted model saving)\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "# Load the model from disk for later use (Flaw: No integrity checks on the loaded model)\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, y_test)\n",
        "print(f'Model Accuracy: {result:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Identify security vulnerabilities\n",
        "As you review the code, look for potential security vulnerabilities such as:\n",
        "\n",
        "Data validation and sanitization: Is the input data validated or sanitized to prevent malicious input?\n",
        "\n",
        "Input validation: Are there any checks on the input data to ensure it meets expected formats or values?\n",
        "\n",
        "Random state and seed management: Is the random state used securely to prevent predictability in model training?\n",
        "\n",
        "Model security: Are there security measures in place to protect the model from tampering or unauthorized access?\n",
        "\n",
        "Encryption: Is sensitive data, such as the trained model, encrypted when stored or transmitted?\n",
        "\n",
        "Integrity checks: Are there mechanisms to verify the integrity of the model when loading it from storage?"
      ],
      "metadata": {
        "id": "n09MrwbzTDVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Document your findings\n",
        "For each vulnerability you identify, document the following:\n",
        "\n",
        "Vulnerability description: Clearly describe the issue you’ve identified.\n",
        "\n",
        "Potential impact: Explain the potential security risks if this vulnerability were exploited.\n",
        "\n",
        "Mitigation strategy: Propose a solution or best practice to mitigate the identified vulnerability.\n",
        "\n",
        "Example documentation:\n",
        "\n",
        "Vulnerability: Lack of data validation and sanitization when loading the dataset.\n",
        "\n",
        "Impact: Malicious users could inject harmful code or corrupted data, leading to data breaches or compromised model integrity.\n",
        "\n",
        "Mitigation: Implement input validation and sanitization routines before processing the data. For example, ensure that the CSV file only contains expected data types and values."
      ],
      "metadata": {
        "id": "jnwct70aTF4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Propose code improvements\n",
        "Based on your findings, propose improvements to the provided code block to enhance its security. Rewrite sections of the code where necessary, incorporating best practices such as:\n",
        "\n",
        "Adding data validation and sanitization.\n",
        "\n",
        "Using secure random state management.\n",
        "\n",
        "Encrypting models before saving them to disk.\n",
        "\n",
        "Implementing integrity checks when loading models.\n",
        "\n",
        "Example code improvement"
      ],
      "metadata": {
        "id": "rxBjMqs-TIol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import hashlib\n",
        "# Validate and sanitize input data\n",
        "def validate_data(df):\n",
        "    # Example validation: Check for null values, correct data types, etc.\n",
        "    if df.isnull().values.any():\n",
        "        raise ValueError(\"Dataset contains null values. Please clean the data before processing.\")\n",
        "    # Additional validation checks can be added here\n",
        "    return df\n",
        "# Load and validate dataset\n",
        "data = validate_data(pd.read_csv('user_data.csv'))\n",
        "# Split the dataset into features and target\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "# Split the data into training and testing sets with a securely managed random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=os.urandom(16))\n",
        "# Train a logistic regression model with added security considerations\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Save the model to disk with encryption\n",
        "filename = 'finalized_model.sav'\n",
        "with open(filename, 'wb') as model_file:\n",
        "    encrypted_model = pickle.dumps(model)\n",
        "    model_file.write(encrypted_model)\n",
        "# Load the model from disk and verify its integrity\n",
        "with open(filename, 'rb') as model_file:\n",
        "    loaded_model = pickle.loads(model_file.read())\n",
        "    if hashlib.sha256(pickle.dumps(loaded_model)).hexdigest() != hashlib.sha256(pickle.dumps(model)).hexdigest():\n",
        "        raise ValueError(\"Model integrity check failed. The model may have been tampered with.\")\n",
        "result = loaded_model.score(X_test, y_test)\n",
        "print(f'Model Accuracy: {result:.2f}')"
      ],
      "metadata": {
        "id": "AQBiel_sTK6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Compile your findings\n",
        "Compile your findings, including the identified vulnerabilities, their potential impacts, proposed mitigation strategies, and the improved code.\n",
        "\n",
        "Conclusion\n",
        "This activity is designed to enhance your ability to identify and address security vulnerabilities in ML code. By completing this exercise, you will gain practical experience in securing AI/ML systems, an essential skill in today’s increasingly data-driven world. Remember, the goal is not just to write functional code, but to ensure that it is secure and robust against potential threats."
      ],
      "metadata": {
        "id": "iJ03M5V5TPLA"
      }
    }
  ]
}