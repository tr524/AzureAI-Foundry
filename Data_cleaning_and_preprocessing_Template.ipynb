{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfmkSvoOMJHX13QeLiAaAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tr524/AzureAI-Foundry/blob/main/Data_cleaning_and_preprocessing_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've just used your data preprocessing tool to clean and prepare a set of dummy data for ML applications. This reading provides a detailed walkthrough of the correct solution, explaining each step in the preprocessing process. By following this guide, you will ensure that your data is properly cleaned, transformed, and ready for use in machine learning models.\n",
        "\n",
        "By the end of this walkthrough, you will be able to:\n",
        "\n",
        "Review and explain the structure of a dataset.\n",
        "\n",
        "Apply data preprocessing techniques, including handling missing values and outliers.\n",
        "\n",
        "Scale numeric features and encode categorical variables for machine learning.\n",
        "\n",
        "Verify the quality of the preprocessed data and ensure its readiness for analysis.\n",
        "\n",
        "\n",
        "\n",
        "Step-by-step guide:\n",
        "\n",
        "#Step 1: Review the dummy data\n",
        "Before we dive into the solution, letâ€™s quickly review the dummy data you worked with:"
      ],
      "metadata": {
        "id": "lOBGS0IgEeQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsB5IQ6hEGLn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy dataset\n",
        "np.random.seed(0)\n",
        "dummy_data = {\n",
        "    'Feature1': np.random.normal(100, 10, 100).tolist() + [np.nan, 200],  # Normally distributed with an outlier\n",
        "    'Feature2': np.random.randint(0, 100, 102).tolist(),  # Random integers\n",
        "    'Category': ['A', 'B', 'C', 'D'] * 25 + [np.nan, 'A'],  # Categorical with some missing values\n",
        "    'Target': np.random.choice([0, 1], 102).tolist()  # Binary target variable\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a pandas DataFrame\n",
        "df_dummy = pd.DataFrame(dummy_data)\n",
        "\n",
        "# Display the first few rows of the dummy dataset\n",
        "print(df_dummy.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "This dataset includes numeric and categorical features, missing values, and an outlier, which simulates common challenges encountered in real-world datasets.\n",
        "\n",
        "Step 2: Apply the preprocessing tool\n",
        "Step 2.1: Handle missing values\n",
        "The first step in preprocessing is addressing any missing values in the dataset. This can be done by either removing missing data or filling in the missing values with an appropriate statistic, such as the mean.\n"
      ],
      "metadata": {
        "id": "8UuPsgv3Ez5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with the mean for numeric columns\n",
        "df_filled = df_dummy.fillna(df_dummy.mean())\n",
        "\n",
        "# Fill missing categorical data with the mode (most frequent value)\n",
        "df_filled['Category'].fillna(df_filled['Category'].mode()[0], inplace=True)\n",
        "\n",
        "print(df_filled.isnull().sum())  # Verify that there are no missing values"
      ],
      "metadata": {
        "id": "xZ9m48k_E5mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "In this solution, numeric missing values are filled with the mean of the respective column, while missing values in the categorical column are filled with the most frequent category (mode). This ensures that no data is lost due to missing values.\n",
        "\n",
        "Step 2.2: Remove outliers\n",
        "Outliers can distort the analysis and negatively impact model performance. You remove them using the Z-score method, which measures how far each data point is from the mean."
      ],
      "metadata": {
        "id": "RvPQJDNLE72f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate Z-scores for numerical features\n",
        "z_scores = np.abs(stats.zscore(df_filled.select_dtypes(include=[np.number])))\n",
        "\n",
        "# Remove rows with any Z-scores greater than 3 (commonly used threshold for outliers)\n",
        "df_no_outliers = df_filled[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "print(df_no_outliers.describe())  # Verify that outliers have been removed"
      ],
      "metadata": {
        "id": "WHj-KW9AE-iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "Z-scores are calculated for all numeric columns, and any rows with Z-scores greater than three are considered outliers and removed. This step ensures that the dataset is free from extreme values that could skew the model.\n",
        "\n",
        "Step 2.3: Scale the data\n",
        "Scaling ensures that all numeric features contribute equally to the analysis, which is important for many ML algorithms, especially those that rely on distance metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "rsBA2EpoFBTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale numeric features using StandardScaler (Z-score normalization)\n",
        "scaler = StandardScaler()\n",
        "df_no_outliers[df_no_outliers.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(df_no_outliers.select_dtypes(include=[np.number]))\n",
        "\n",
        "print(df_no_outliers.head())  # Verify that the data has been scaled"
      ],
      "metadata": {
        "id": "V9wUZYTgFEcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "The StandardScaler scales numeric features so that they have a mean of zero and a standard deviation of one. This transformation helps improve the performance of many ML algorithms.\n",
        "\n",
        "Step 2.4: Encode categorical variables\n",
        "ML models require numeric input, so categorical variables must be converted into a numerical format using one-hot encoding."
      ],
      "metadata": {
        "id": "457LXDBrFGbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the categorical feature\n",
        "df_encoded = pd.get_dummies(df_no_outliers, columns=['Category'])\n",
        "\n",
        "print(df_encoded.head())  # Verify that the categorical variable has been encoded"
      ],
      "metadata": {
        "id": "zpbBgEsXFJA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "The pd.get_dummies() function converts the categorical column into multiple binary columns, each representing a category. This allows the categorical data to be used in machine learning models.\n",
        "\n",
        "Step 2.5: Save the preprocessed data\n",
        "Finally, the cleaned and preprocessed dataset is saved to a new comma-separated values (CSV) file, making it ready for use in further analysis or model training."
      ],
      "metadata": {
        "id": "o6vciVJHFMjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the preprocessed DataFrame to a CSV file\n",
        "df_encoded.to_csv('preprocessed_dummy_data.csv', index=False)\n",
        "\n",
        "print('Preprocessed data saved as preprocessed_dummy_data.csv')"
      ],
      "metadata": {
        "id": "XB3eny8pFOrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "The preprocessed data is saved to a file named preprocessed_dummy_data.csv. This file can now be used as input for ML algorithms, ensuring that the data is clean, consistent, and properly formatted.\n",
        "\n",
        "Step 3: Verify the data and perform a quality check\n",
        "After completing the preprocessing steps, it's important to verify that the data has been processed correctly. You should check for the following:\n",
        "\n",
        "No missing values\n",
        "Ensure that all missing values have been handled."
      ],
      "metadata": {
        "id": "L9D67FJfFRW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.isnull().sum())"
      ],
      "metadata": {
        "id": "4Zs4P_cqFT6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No outliers\n",
        "Confirm that outliers have been removed."
      ],
      "metadata": {
        "id": "m4suYc6uFcJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.describe())"
      ],
      "metadata": {
        "id": "aUvmV-C8Fg2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proper scaling\n",
        "Check that the numeric features have been scaled appropriately."
      ],
      "metadata": {
        "id": "g8b25NhbFjAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.head())"
      ],
      "metadata": {
        "id": "lUZB--HIFl55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct encoding\n",
        "Ensure that categorical variables have been properly encoded."
      ],
      "metadata": {
        "id": "vyyq2PYiFpBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.columns)"
      ],
      "metadata": {
        "id": "H8Qhs98lFtI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "These verification steps help ensure that the preprocessing has been carried out correctly, reducing the risk of errors in subsequent analysis or modeling.\n",
        "\n",
        "Conclusion\n",
        "By following this walkthrough, youâ€™ve successfully preprocessed a set of dummy data using your data preprocessing tool. The steps included handling missing values, removing outliers, scaling numeric features, and encoding categorical variables. The resulting preprocessed data is now ready for ML applications.\n",
        "\n",
        "This process is essential in real-world data science workflows, as it ensures that the data fed into ML models is of high quality and properly formatted. As you continue to work with different datasets, you can adapt and refine these preprocessing steps to meet the specific needs of each project, leading to more accurate and reliable models.\n",
        "\n"
      ],
      "metadata": {
        "id": "2A3IyYmwFvlW"
      }
    }
  ]
}